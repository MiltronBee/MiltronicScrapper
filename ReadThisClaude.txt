A Technical Blueprint for a Robust Spanish-Language Web Scraping Framework for High-Quality Corpus GenerationSection 1: Architectural Blueprint for a Production-Grade Scraping FrameworkThis section establishes the foundational architectural principles and project structure for a scalable and maintainable web scraping system. The design prioritizes modularity, resilience, and extensibility, moving beyond simple scripting to engineer a framework capable of large-scale, long-term data acquisition for demanding Natural Language Processing (NLP) applications.1.1. The Philosophy: From Monolithic Scripts to a Modular FrameworkThe task of web scraping, particularly for generating high-quality corpora, presents challenges that escalate non-linearly with scale. A simple, monolithic script, while adequate for extracting data from a single source or for a short-lived task, inevitably encounters significant limitations when applied to a large-scale, continuous operation. Common failure points include brittleness in the face of website layout changes, difficulty in debugging isolated issues, poor reusability of code, and an inability to scale efficiently.1 Websites are inherently unstable targets; their structure can change without notice, breaking hardcoded selectors and parsing logic.4 In a monolithic design, a change to one target site can necessitate complex and high-risk modifications throughout the entire codebase, leading to a system that is difficult and costly to maintain.To overcome these challenges, this blueprint adopts the principles of modular programming: separation of concerns, encapsulation, and reusability. By architecting the system as a collection of distinct, interconnected components, each with a single, well-defined responsibility, we can build a framework that is robust, adaptable, and easy to manage over time.2The user's requirement to scale to over 10,000 documents and to easily integrate new sources is a fundamental architectural driver, not merely a feature request. This scale mandates a design that anticipates and isolates change. A modular, class-based architecture is the direct solution to this challenge.1 In this paradigm, logic specific to a single website (e.g., custom CSS selectors for fallback extraction) is encapsulated within a dedicated module or configuration entry. For instance, if the layout of dof.gob.mx is updated by its administrators, only the configuration related to that specific source needs to be adjusted. The core components responsible for fetching web pages, managing politeness protocols, saving files, and logging operations remain entirely untouched. This architectural choice directly enables the long-term maintainability and operational stability required for the specified scale, establishing a clear causal link between a modular design and the success of the project.1.2. Core Components of the FrameworkThe proposed framework is built upon a class-based design that logically separates the distinct stages of the web scraping pipeline. This structure ensures that each component can be developed, tested, and maintained independently.The key components of the framework are:Orchestrator: This is the central controller and main entry point of the application. It is responsible for initializing all other components, reading the master configuration, managing the queue of URLs to be processed, and overseeing the entire scraping workflow from start to finish. It coordinates the actions of the other modules to execute the scraping job as defined in the configuration files.ConfigManager: A dedicated utility class for managing all external configuration. It loads, validates, and provides a clean, accessible interface to the settings defined in config.yaml and sources.yaml. This component ensures that all operational parameters are decoupled from the application logic, allowing for easy modification without code changes.7Scraper: This is the network-facing component, responsible for all HTTP/S interactions. Its duties include fetching raw HTML content, managing requests.Session objects for connection pooling, and rigorously enforcing politeness protocols such as respecting robots.txt, throttling request rates, and rotating User-Agent headers.1 It will also house the logic for switching between static (requests) and dynamic (Playwright) fetching mechanisms.Extractor: The data processing engine of the framework. It receives raw HTML from the Scraper and applies a multi-layered strategy to extract, purify, and validate the target text content. This component will primarily leverage trafilatura for its powerful main-content extraction capabilities, with a fallback to BeautifulSoup for more surgical, site-specific rules.10Saver: The persistence layer of the framework. It is responsible for all disk I/O related to the final output. This includes handling the specified filename convention (SOURCE_YYYYMMDD_HASH.txt), managing the output directory structure, writing cleaned text to UTF-8 encoded files, and performing content-based deduplication to prevent redundant files in the corpus.10StateManager: A critical component for ensuring the robustness and continuity of long-running scraping jobs. It manages the state of every URL (e.g., pending, processed, failed) and implements a checkpointing system that allows the framework to be stopped and resumed without losing progress.14exceptions.py: A dedicated module for defining custom exception classes. This allows for more granular error handling and clearer, more informative logging throughout the application. For example, RobotsBlockedError, ContentValidationError, or LanguageMismatchError provide more context than generic exceptions.1.3. Proposed Project Directory StructureA well-organized directory structure is essential for maintainability and clarity. The following structure is proposed to logically group related components and separate application code from configuration, data, and logs. This layout is a synthesis of best practices observed in robust Python applications and is designed to support every feature requested in the prompt.2corpus_scraper/
├── corpus_scraper/
│   ├── __init__.py
│   ├── orchestrator.py    # Main control logic and workflow management
│   ├── config_manager.py  # Handles loading and validation of config.yaml
│   ├── scraper.py         # Handles all network requests, politeness protocols
│   ├── extractor.py       # Handles text extraction, cleaning, and validation
│   ├── saver.py           # Handles file writing, naming, and deduplication
│   ├── state_manager.py   # Manages checkpointing and resume logic
│   └── exceptions.py      # Custom exception classes for granular error handling
├── main.py                # Main script entry point to start the scraper
├── config.yaml            # Main configuration file for the framework
├── sources.yaml           # List of target URLs, sitemaps, and source-specific settings
├── requirements.txt       # Project dependencies for reproducible environments
├── README.md              # Documentation for setup, configuration, and usage
└── data/
    ├── corpus_raw/        # Final output directory for clean.txt files
    │   ├── dof/
    │   └──...
    ├── html_raw/          # Optional directory for storing raw HTML for debugging
    └── logs/              # Directory for log files
        └── scraper.log
    └── state/             # Directory for state management and checkpointing
        └── scraper_state.db
This structure clearly separates the core application logic (within the inner corpus_scraper/ directory) from the runnable script (main.py), configuration files (.yaml), documentation (README.md), and all data artifacts (data/). This separation is crucial for clean development, testing, and potential deployment.Section 2: Configuration-Driven Operations with YAMLTo build a truly flexible and maintainable framework, it is imperative to decouple the operational parameters from the core application logic. This allows for adjustments to scraping targets, politeness settings, and validation rules without necessitating any changes to the Python code. This section details the design of a configuration-driven system using YAML, managed by a dedicated ConfigManager class.2.1. Why YAML? A Comparison with JSON and HardcodingWhile parameters can be hardcoded directly into scripts, this approach is highly inflexible and completely unsuitable for a production-grade framework. Any change, no matter how minor, requires a code modification, testing, and redeployment. The choice between external configuration formats primarily comes down to JSON and YAML.JSON (JavaScript Object Notation) is a lightweight and widely supported data-interchange format. However, its strict syntax (e.g., no comments, required double quotes) makes it less than ideal for configuration files that are intended to be read and edited by humans.YAML (YAML Ain't Markup Language), in contrast, is designed specifically for human readability.16 Its key advantages for this project include:Readability: The clean, indentation-based syntax is familiar to Python developers and is less cluttered than JSON.16Comments: YAML supports comments, which are invaluable for documenting configuration options and explaining the purpose of specific settings. This is a feature entirely absent from JSON.9Conciseness: It offers a more concise syntax for representing lists and dictionaries.Given these benefits, YAML is the superior choice for the framework's configuration files.9 The PyYAML library will be used as the standard, robust tool for parsing these files within the Python application.82.2. Designing the ConfigManager and SchemaThe ConfigManager class will serve as the single point of access for all configuration data. Upon initialization, it will load and parse both config.yaml and sources.yaml. It should also perform basic schema validation to ensure that all required keys are present, preventing runtime errors due to misconfiguration.2.2.1. The config.yaml SchemaThis file acts as the framework's central control panel. It defines the global behavior of the scraper.YAML# config.yaml: Global configuration for the scraping framework

# --- Polite Scraping Settings ---
# Defines the behavior of the network-facing Scraper component.
politeness:
  # Base delay in seconds between consecutive requests to the same domain.
  request_delay: 2.0
  # Random jitter to add to the delay (e.g., a value of 1.0 adds a random delay between 0 and 1 seconds).
  jitter: 1.0
  # Request timeout in seconds.
  timeout: 30
  # Number of retry attempts for failed requests (e.g., timeouts, 5xx errors).
  retry_attempts: 3
  # Path to a file containing a list of User-Agent strings to rotate through.
  user_agent_file: 'user_agents.txt'

# --- Extraction Engine Settings ---
# Configures the text extraction strategy.
extraction:
  # Primary engine for main content extraction.
  primary_engine: 'trafilatura'
  # Fallback engine for cases where the primary fails or for site-specific rules.
  fallback_engine: 'beautifulsoup'
  # Headless browser engine for JavaScript-heavy sites.
  headless_engine: 'playwright'

# --- Validation and Quality Control ---
# Rules to ensure the quality of the extracted text.
validation:
  # Minimum number of words required for a document to be saved.
  min_word_count: 200
  # Optional: Minimum number of sentences required. Set to 0 to disable.
  min_sentence_count: 5
  # ISO 639-1 code for the required language. Documents in other languages will be skipped.
  required_language: 'es'
  # Confidence threshold (0.0 to 1.0) for the language detection model.
  lang_detect_confidence: 0.90

# --- Storage and Persistence Settings ---
# Defines where and how data is stored.
storage:
  # Root directory for all output.
  output_dir: 'data/corpus_raw'
  # Directory for log files.
  log_dir: 'data/logs'
  # Directory for state management files (e.g., checkpoint database).
  state_dir: 'data/state'
  # If true, saves the raw HTML of each processed page for debugging.
  save_raw_html: false
  # Directory to store raw HTML if save_raw_html is true.
  raw_html_dir: 'data/html_raw'

# --- Concurrency Settings ---
# Configures multithreading for faster scraping.
concurrency:
  # Number of worker threads to use for fetching and processing URLs.
  # A value of 1 disables multithreading.
  num_threads: 4
2.2.2. The sources.yaml SchemaThis file defines the specific targets for the scraper. It is designed to be simple to edit, allowing new sources to be added with minimal effort.YAML# sources.yaml: List of target websites for corpus generation.

sources:
  - name: 'dof'
    base_url: 'https://dof.gob.mx'
    # The sitemap provides a structured list of URLs, which is more efficient than crawling.
    sitemap_url: 'https://dof.gob.mx/sitemap.xml' # Example, actual URL may differ
    # No engine override needed, will use default from config.yaml

  - name: 'scjn'
    base_url: 'https://www.scjn.gob.mx'
    # This site may require crawling if a sitemap is not available or comprehensive.
    start_urls:
      - 'https://www.scjn.gob.mx/multimedia'
      - 'https://www.scjn.gob.mx/conoce-la-corte'
    # Optional: Define site-specific CSS selectors for fallback extraction if trafilatura fails.
    fallback_selector: 'div.main-content-article'

  - name: 'unam'
    base_url: 'https://www.unam.mx'
    sitemap_url: 'https://www.unam.mx/sitemap.xml' # Example
    # Example of a site that might require a headless browser for some sections.
    # The framework will use Playwright for URLs from this source.
    engine: 'playwright'
2.3. The Power of Decoupling Configuration from LogicThe decision to implement a fully configuration-driven architecture is not merely a technical convenience; it is a strategic choice that fundamentally enhances the framework's utility and empowers a multidisciplinary team. The user's prompt specifies a team comprising not just engineers but also "writers, PhD researchers, analysts... and other domain specialists" [User Query]. For such a team, a system where operational parameters are hardcoded is a bottleneck, requiring constant intervention from a developer for even minor adjustments.By externalizing all key parameters—from the list of target websites and sitemaps in sources.yaml to the politeness settings and validation thresholds in config.yaml—we effectively decouple the what from the how.7 This separation has profound implications for workflow efficiency. A linguist or a legal analyst, for example, can now directly add a new government portal to sources.yaml or tighten the min_word_count parameter in config.yaml to refine the quality of the legal corpus. These actions, which directly impact the research outcomes, can be performed without any knowledge of Python or the underlying complexity of the scraping logic.This architectural approach transforms the framework from a siloed developer's tool into a shared, collaborative platform. It democratizes control over the data acquisition process, allowing domain experts to iterate on the corpus generation strategy independently. This agility significantly accelerates the overall research and development cycle, as the feedback loop between data acquisition and analysis is shortened. The framework becomes a more powerful and responsive instrument in the hands of the entire team, directly contributing to the primary goal of generating a high-quality, specialized text corpus.Section 3: The Politeness Protocol: Engineering a Responsible and Resilient ScraperA successful large-scale web scraping project is one that can run for extended periods without being blocked or disrupting the services it relies on. This requires a proactive and sophisticated approach to "polite" scraping. The Scraper class will be engineered to be a responsible web citizen, incorporating robust mechanisms for robots.txt compliance, advanced request throttling, and comprehensive header management. These features are not optional; they are essential for the long-term viability and success of the data acquisition effort.3.1. robots.txt: Beyond Simple ComplianceThe Robots Exclusion Protocol (REP) is a standard that allows website administrators to communicate with web crawlers and other automated agents, specifying which parts of the site should not be accessed.17 Adherence to robots.txt is the first and most fundamental principle of ethical scraping.19Our framework must begin any interaction with a new domain by fetching and parsing its robots.txt file. For example, an analysis of dof.gob.mx/robots.txt reveals specific Disallow directives for dynamic PHP pages like nota_detalle.php? and various index.php URLs with query parameters.21 The scraper must strictly honor these rules. In cases like scjn.gob.mx, where the robots.txt file may be inaccessible (returning a 404 or other error), the framework should adopt a conservative default policy, which is typically to proceed with crawling while maintaining a respectful request rate, as Google's crawlers do.22For parsing these files, Python's standard library offers urllib.robotparser. However, it is based on an older draft of the REP and lacks support for modern, widely used directives.24 For instance, it does not natively understand the Crawl-delay directive, which is a crucial parameter for throttling, nor does it properly handle wildcards (*) or end-of-string markers ($) in path rules, which are part of the newer standard (RFC 9309) and used by major search engines.25A superior alternative is the third-party library protego. It is a pure-Python parser designed with modern conventions in mind.27protego correctly interprets Crawl-delay, Request-rate, and advanced wildcard matching. By using protego, the framework can more accurately interpret the rules set by webmasters, reducing the risk of accidental non-compliance.The Scraper class will therefore use protego to parse the robots.txt for each domain. It will cache the parsed rules to avoid re-fetching the file for every request and will implement a method can_fetch(user_agent, url) that consults these rules before initiating any download.Python# Example snippet for the Scraper class using protego
import requests
from protego import Protego

class Scraper:
    def __init__(self, user_agent):
        self._user_agent = user_agent
        self._robot_parsers = {}

    def _get_robot_parser(self, url):
        domain = urlparse(url).netloc
        if domain not in self._robot_parsers:
            robots_url = f"https://{domain}/robots.txt"
            try:
                response = requests.get(robots_url, timeout=10)
                response.raise_for_status()
                self._robot_parsers[domain] = Protego.parse(response.text)
            except requests.RequestException:
                # If robots.txt is inaccessible, create a parser that allows all
                self._robot_parsers[domain] = Protego.parse("")
        return self._robot_parsers[domain]

    def can_fetch(self, url):
        parser = self._get_robot_parser(url)
        return parser.can_fetch(url, self._user_agent)
3.2. Advanced Throttling: Mimicking Human BehaviorSimply inserting a fixed delay, such as time.sleep(1), between requests is a rudimentary form of throttling that can still create predictable, machine-like access patterns.28 Anti-bot systems are adept at detecting such regularity.29 A more robust strategy involves variability and adaptability.Randomized Delays with Jitter: The framework will implement a randomized delay mechanism. Based on the request_delay and jitter values in config.yaml, the actual delay between requests will be calculated as delay = request_delay + random.uniform(0, jitter). This introduces a degree of randomness that better mimics human browsing behavior and makes the scraper's access pattern harder to fingerprint.30Exponential Backoff for Errors: Network requests can fail for transient reasons, such as temporary server overloads (HTTP 503), rate limiting (HTTP 429), or gateway timeouts (HTTP 504).29 A resilient scraper should not give up after a single failure. The tenacity library provides a powerful and elegant way to implement automated retry logic.34 The Scraper's fetch method will be decorated with tenacity.retry, configured to:Retry only on specific, transient HTTP error codes (5xx, 429).Employ an exponential backoff strategy, where the wait time between retries increases after each failure (e.g., 2s, 4s, 8s). This gives the server time to recover.Incorporate random jitter into the backoff delay to prevent multiple scraper instances from retrying in synchronized "thundering herds."Stop after a configurable number of attempts (retry_attempts from config.yaml).This combination of proactive throttling and reactive, intelligent retries makes the Scraper component highly resilient to common network and server issues, which is critical for the success of long-running jobs.3.3. User-Agent and Header RotationWeb servers identify clients primarily through the User-Agent string sent in the HTTP request headers. The default User-Agent for libraries like Python's requests immediately flags the client as a script.35 It is therefore essential to spoof this header by using strings from real, modern web browsers.36However, simply rotating User-Agent strings is often insufficient. Sophisticated anti-scraping systems employ a more holistic approach to fingerprinting clients. They check not just the User-Agent but also the consistency of the entire set of HTTP headers.32 For example, a request claiming to be from a modern Chrome browser on Windows but lacking the sec-ch-ua (User-Agent Client Hints) headers, or sending a generic Accept: */* header instead of the specific value Chrome sends, is an easily detectable anomaly.To build a truly stealthy and robust framework, we must move beyond rotating individual User-Agent strings and instead rotate complete, consistent header profiles. The framework will implement this advanced strategy as follows:Header Profile Definition: A separate file (e.g., header_profiles.json) will define a list of profiles. Each profile will contain a valid User-Agent string along with the corresponding Accept, Accept-Language, Sec-CH-UA, and other headers that are characteristic of that specific browser and OS combination.32Dynamic Selection: Before each request, the Scraper will randomly select one of these profiles.Request Construction: The entire set of headers from the selected profile will be used to construct the request.Dynamic Generation (Optional): For even greater variety, a library like fake-useragent can be integrated.38 While it primarily generates User-Agent strings, it can be used as a key to look up corresponding header sets, ensuring that the generated User-Agents are always up-to-date and reflect real-world browser usage statistics.This holistic approach to header management presents a much more convincing and human-like fingerprint to the target server, significantly reducing the probability of being blocked and increasing the overall robustness of the scraping operation. This level of detail in mimicking browser behavior is a key differentiator between a simple script and a production-ready data acquisition system.Section 4: A Multi-Layered Strategy for Content Extraction and PurificationThe core task of this framework is to extract high-quality, formal text. This requires a sophisticated Extractor component that can reliably separate the valuable content (the main body of an article, a legal document, etc.) from the surrounding boilerplate (navigation menus, advertisements, footers, scripts). This section details a multi-layered strategy that combines the power of specialized extraction libraries, pre-emptive HTML sanitization, and surgical fallback mechanisms to achieve maximum quality and robustness.4.1. Primary Extraction with trafilaturaWhile general-purpose HTML parsers like BeautifulSoup are powerful tools for navigating a document tree, they are not inherently "aware" of what constitutes the main content of a page.11 Using them for this purpose requires the developer to write site-specific rules to identify the correct container elements (e.g., <div class="article-body">), a process that is brittle and time-consuming.trafilatura is a library purpose-built for this exact problem. It employs a set of heuristics and algorithms to automatically identify and extract the primary text content of a web page, while intelligently filtering out boilerplate, navigation, and other noise.12 Its key advantages make it the ideal primary extraction engine for this framework:Turnkey Operation: It works out-of-the-box on a vast majority of websites without requiring any site-specific configuration.11High Accuracy: Benchmarks and real-world use show that it consistently outperforms other open-source libraries in extracting clean text with minimal noise.40Robustness: It is designed to handle messy, old, and poorly formatted HTML, which is common on the web.39Metadata Extraction: In addition to the main text, trafilatura can extract valuable metadata such as the author, publication date, and title, which can be useful for enriching the corpus.40The Extractor will first pass the raw HTML to trafilatura.extract(). This single function call will, in most cases, return the clean, formatted text required for the corpus, significantly simplifying the extraction pipeline.394.2. Pre-emptive HTML Sanitization for Corpus PurityBefore text extraction even begins, the quality and reliability of the entire process can be significantly improved by first sanitizing the raw HTML. Web pages are often laden with <script> tags, CSS stylesheets, inline event handlers (onclick), and other elements that are irrelevant to the text content and can interfere with parsing algorithms. A pre-emptive sanitization step removes this noise, providing a cleaner and safer input for the extraction engines.The choice of a sanitizer library is critical. The primary distinction is between blocklist-based and allowlist-based approaches. A blocklist approach (trying to remove known "bad" things) is inherently insecure and prone to failure, as new web technologies and attack vectors emerge. An allowlist approach (only permitting known "good" things) is fundamentally more secure and reliable.Table 1: Comparison of Python HTML Sanitizer LibrariesLibrarySecurity ModelKey Features & PhilosophyPerformanceMaintenance Statuslxml.html.cleanBlocklistRemoves a predefined list of "bad" tags and attributes (e.g., script, style). Fast but fundamentally insecure.HighDeprecated for security use. Moved to a separate lxml_html_clean package as of lxml 5.2.0.41bleachAllowlistSecurity-focused library from Mozilla. Requires explicit definition of all allowed tags and attributes. Based on html5lib.ModerateActively maintained for security, but the project has stated it is largely feature-complete and deprecated new feature development.41html-sanitizerAllowlist"Opinionated" sanitizer built on lxml. Not only sanitizes but also normalizes HTML for consistency (e.g., converts <b> to <strong>, normalizes whitespace).HighActively maintained, with a focus on producing clean, consistent HTML for web contexts.44Analysis and Recommendation:The lxml.html.clean module, while fast, uses a blocklist approach that has been the source of several security vulnerabilities and is now officially deprecated for any security-sensitive use case.41 It is therefore unsuitable for this framework.Both bleach and html-sanitizer are excellent, secure, allowlist-based libraries.42 However, for the specific goal of creating a high-quality NLP corpus, html-sanitizer has a distinct advantage. Its "opinionated" philosophy aligns perfectly with the project's objectives. It goes beyond simple sanitization to perform transformations that enforce consistency, such as normalizing whitespace, merging adjacent tags, and converting presentational tags (<i>, <b>) into semantic ones (<em>, <strong>).44 This pre-processing step creates a more uniform and predictable HTML structure, which in turn simplifies the task for the downstream trafilatura extractor and contributes to a cleaner final text output.Therefore, the Extractor will first pass the raw HTML through a configured html-sanitizer instance before attempting text extraction.4.3. Surgical Fallback with BeautifulSoupNo automated tool is infallible. There will be instances where trafilatura fails to extract content or extracts an incomplete or incorrect section of the page. To handle these cases and maximize data yield, the framework must include a fallback mechanism.If the primary extraction with trafilatura yields content that fails validation (e.g., word count is too low), the Extractor will switch to its fallback engine: BeautifulSoup. This is where the power of site-specific rules comes into play. The sources.yaml file can contain an optional fallback_selector key for each source. This key will hold a CSS selector that precisely targets the main content container for that specific site.Python# Example logic within the Extractor class
from bs4 import BeautifulSoup
import trafilatura

class Extractor:
    #... (initialization)...

    def extract(self, html_content, source_config):
        # Step 1: Pre-emptive sanitization
        sanitized_html = self.sanitizer.sanitize(html_content)

        # Step 2: Primary extraction with trafilatura
        text = trafilatura.extract(sanitized_html, include_comments=False, include_tables=False)

        # Step 3: Validate and potentially fallback
        if not self._is_valid(text):
            fallback_selector = source_config.get('fallback_selector')
            if fallback_selector:
                # Attempt fallback extraction with BeautifulSoup
                soup = BeautifulSoup(sanitized_html, 'lxml')
                content_area = soup.select_one(fallback_selector)
                if content_area:
                    text = content_area.get_text(separator='\n', strip=True)
        
        return text
This multi-layered approach provides the best of both worlds: the speed and convenience of automated extraction with trafilatura for the majority of pages, and the precision and power of BeautifulSoup with custom selectors for the edge cases and difficult sites.474.4. Handling Dynamic Content with PlaywrightA significant portion of the modern web relies heavily on JavaScript to render content dynamically. For such websites, a simple HTTP request using a library like requests will only retrieve the initial HTML skeleton, which is often devoid of the actual content. To scrape these sites, a full browser engine is required to execute the JavaScript and render the final page.The choice between a static fetcher like requests and a headless browser like Playwright involves a critical trade-off between speed and capability. requests is exceptionally fast and has a very low resource footprint, making it ideal for the vast majority of static websites.1Playwright, by contrast, can automate a real browser (Chromium, Firefox, WebKit), render complex JavaScript, and simulate user interactions like clicks and scrolls, but at the cost of significantly higher CPU, memory usage, and slower execution time.52A high-performance framework must not be globally bottlenecked by using a headless browser for every single request. The architecture must allow for a surgical application of this powerful but expensive tool. The Scraper component will be designed as a factory that selects the appropriate fetching mechanism on a per-source basis.Configuration-Driven Selection: The sources.yaml file will control this behavior. If a source entry includes the flag engine: playwright, the Orchestrator will instruct the Scraper to use the PlaywrightFetcher for all URLs belonging to that source. Otherwise, the default, high-speed RequestsFetcher will be used.PlaywrightFetcher Implementation: This sub-component will manage the browser lifecycle (launching, creating contexts, and closing). To optimize performance, it will implement several strategies:Resource Blocking: It will be configured to abort requests for unnecessary resources like images, fonts, and tracking scripts, which can significantly speed up page load times.56Context Management: It will leverage browser contexts to isolate sessions between different scraping tasks, improving stability.Integration with requests Session: For scenarios requiring a mix of static and dynamic requests within a single user session (e.g., logging in via requests and then scraping a dynamic page with Playwright), it's possible to share state. Cookies from a requests.Session object can be extracted and injected into a Playwright browser context, ensuring session continuity.57This design ensures that the framework applies the right tool for the job, maximizing overall throughput by using the lightweight requests library wherever possible and reserving the powerful but resource-intensive Playwright only for the sites that explicitly require it.Section 5: The Quality Assurance Pipeline: Validation and FilteringExtracting text is only half the battle. To generate a corpus of sufficient quality for fine-tuning Large Language Models (LLMs), a rigorous post-extraction validation and filtering pipeline is essential. This stage ensures that every document saved to the corpus meets predefined criteria for language, length, and structure. This section details the components of this quality assurance pipeline.5.1. High-Fidelity Language IdentificationThe primary goal of this project is to create a corpus of formal Mexican Spanish. Therefore, accurately identifying and discarding documents written in other languages is a non-negotiable requirement. This check must be performed on every successfully extracted piece of text. The choice of language detection library involves a trade-off between accuracy, performance, and language coverage.Table 2: Comparison of Python Language Detection LibrariesLibrarySpanish Accuracy (F1-Score)Throughput (sentences/sec)Memory UsageKey Characteristicsfasttext~0.97 (High) 61~112,000 (Very High) 62136 MB (Large Model) 62Best overall balance of speed and accuracy for major languages. Supports 176 languages.62langidLower than fasttext~1,200 (Moderate) 6236 MB (Low) 62Good performance, but less accurate than fasttext. Supports 97 languages.64pycld2Lower than fasttext~258,000 (Highest) 620.24 MB (Lowest) 62Extremely fast and lightweight, but accuracy is lower, especially on short texts. From Google.62langdetectHigh~230 (Very Low) 6269 MB (Moderate) 62Accuracy is good but performance is very slow. Can be non-deterministic without seeding.62Analysis and Recommendation:The data clearly indicates that fasttext offers the best combination of features for this production-grade framework. While pycld2 is faster, its lower accuracy is a significant drawback when corpus purity is the goal.62langdetect is far too slow for a large-scale pipeline and suffers from non-determinism, making it unreliable.62langid is a competent library, but fasttext provides superior accuracy for major languages like Spanish with significantly higher throughput.63The fasttext model (lid.176.bin) demonstrates an F1-score of approximately 0.97 for Spanish on the Tatoeba dataset, which consists of short, challenging sentences.61 This high level of accuracy, combined with its exceptional speed, makes it the optimal choice. The Extractor will use fasttext to check the language of every extracted text. If the detected language is not 'es' or if the confidence score is below the lang_detect_confidence threshold set in config.yaml, the document will be discarded.5.2. Content-Based Validation with spaCyAfter confirming the document is in Spanish, the framework must validate its content to ensure it is substantial enough for NLP tasks. This involves checking against the min_word_count and min_sentence_count thresholds from the configuration.spaCy is an industrial-strength NLP library in Python, perfect for this task.68 However, a full spaCy pipeline, which includes a tagger, parser, and named entity recognizer (NER), is computationally expensive and unnecessary for simple word and sentence counting.69 To achieve maximum performance, the framework will employ one of two optimized strategies for loading a spaCy model:Using a Blank Model: For the sole purpose of tokenization and sentence segmentation, a blank Spanish model can be initialized with nlp = spacy.blank("es"). This creates a pipeline with only a tokenizer and default rule-based sentence segmentation, offering the fastest possible processing speed.69Disabling Unnecessary Components: If a pre-trained model is preferred (for its potentially more sophisticated tokenizer rules), it can be loaded with all non-essential components disabled: nlp = spacy.load("es_core_news_sm", disable=["parser", "ner", "tagger", "lemmatizer", "attribute_ruler"]). This approach still provides the benefits of the trained tokenizer while avoiding the significant overhead of the deep learning components.69spaCy's sentence segmentation is particularly robust for formal text, as its models are trained on large news and web corpora.72 The Spanish models (es_core_news_*) achieve an F-score of 0.98-0.99 for sentence segmentation, making them far superior to naive methods like splitting on periods, which would fail on abbreviations common in formal documents.73While spaCy's tokenizer is generally excellent, some specific linguistic conventions, such as the use of em-dashes in Spanish dialogue without surrounding spaces, can present challenges.74 For the target sources (formal government and university texts), these issues are less likely to be prevalent. However, the framework's modular design allows for the future addition of custom tokenization rules or sentence boundary detection logic if specific sources require it, without altering the core validation process.71The validation step will process the cleaned text with the optimized spaCy pipeline, count the number of tokens and sentences, and discard any document that does not meet the configured minimums. This final quality gate ensures that only rich, substantial documents are saved to the corpus.Section 6: Ensuring Continuity and ScaleFor a scraping framework designed to process over 10,000 documents, robustness against interruptions and the ability to scale performance are not bonus features—they are core requirements. This section details the implementation of the Saver and StateManager components, along with a concurrency model, to ensure data integrity, enable job resumption, and maximize throughput.6.1. Data Persistence and DeduplicationThe Saver class is responsible for all interactions with the file system for the final corpus output. Its design must prioritize data integrity and efficiency.Filename Convention and Hashing: As per the user's request, the Saver will implement the SOURCE_YYYYMMDD_HASH.txt naming convention. The SOURCE will be derived from the source's name in sources.yaml, YYYYMMDD will be the date of processing, and HASH will be a cryptographic hash (e.g., SHA-256) of the final, cleaned text content. Using the content's hash as part of the filename provides an elegant and efficient mechanism for data deduplication. Before writing a new file, the Saver can check if a file with an identical hash already exists in the target directory. If it does, the new document is a duplicate and can be discarded, preventing redundant data in the corpus and saving disk space.Atomic Writes: A critical failure mode in any data-writing application is a crash that occurs mid-write, resulting in a corrupted, incomplete file. To prevent this, the Saver must implement atomic writes. Instead of writing directly to the final destination file, the process is as follows:The cleaned text is written to a temporary file in the same directory (e.g., dof_20231027_abc123.txt.tmp).Once the write operation is complete and the file is closed, the temporary file is renamed to its final destination name (dof_20231027_abc123.txt).File system rename operations are typically atomic, meaning this final step is instantaneous and cannot be interrupted. This ensures that the main output directory will only ever contain complete, valid files.146.2. A Resilient Checkpointing and Resumption StrategyLong-running scraping jobs are susceptible to interruptions from network failures, server errors, or even planned system maintenance. The ability to resume a job from where it left off is crucial for efficiency and data completeness. A naive checkpointing strategy, such as logging every processed URL to a simple text file, is fraught with peril.15 Such a file is highly susceptible to corruption if a crash occurs during a write operation. Furthermore, managing safe, concurrent access to a single text file from multiple threads is complex and prone to race conditions.A vastly superior and more robust strategy is to use a transactional database for state management. For this framework, a lightweight, file-based SQLite database is the ideal choice. It requires no separate server process, is extremely fast for this use case, and, most importantly, its transactions are atomic. This means that updates to the state are guaranteed to either complete fully or not at all, completely eliminating the risk of a corrupted state file, even in the event of a sudden crash.76The StateManager class will encapsulate all interactions with this SQLite database, which will be stored at data/state/scraper_state.db. It will manage a single table, url_status, with the following schema:SQLCREATE TABLE IF NOT EXISTS url_status (
    url_hash TEXT PRIMARY KEY,
    url TEXT NOT NULL,
    source TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending', -- pending, processing, completed, failed
    last_attempt TIMESTAMP,
    attempts INTEGER NOT NULL DEFAULT 0,
    error_message TEXT
);
The workflow for checkpointing and resumption is as follows:Initialization: On startup, the Orchestrator asks the StateManager for a batch of URLs to process. The StateManager queries the database for URLs whose status is 'pending' or 'failed' (and where attempts is less than the configured maximum). It then updates the status of these URLs to 'processing' within a single transaction.Processing: As the Orchestrator processes each URL, it reports the outcome back to the StateManager.On success, the status is updated to 'completed'.On a recoverable failure (e.g., timeout), the status is set back to 'failed', the attempts count is incremented, and the error message is logged.On a non-recoverable failure (e.g., 404 Not Found), the status is updated to a terminal state like 'failed_permanent'.Resumption: If the script is stopped and restarted, the initialization step automatically picks up where it left off by querying for non-completed URLs. No progress is lost, and no URLs are processed twice.This database-centric approach provides a professional, scalable, and crash-proof solution for state management, far exceeding the capabilities of simple file-based logging.146.3. Scaling with Thread-Safe ConcurrencyTo accelerate the scraping process, especially when dealing with thousands of URLs and network latency, the framework will implement multithreading. Since web scraping is a classic I/O-bound task (the program spends most of its time waiting for network responses), Python's concurrent.futures.ThreadPoolExecutor is the perfect tool. It allows multiple network requests to be in flight simultaneously, effectively hiding the latency of each individual request and dramatically increasing overall throughput. The Global Interpreter Lock (GIL), which can be a bottleneck for CPU-bound tasks, is not a significant hindrance here, as it is released by threads during I/O operations.77The Orchestrator will manage the thread pool. Instead of processing URLs in a simple loop, it will submit a "scrape task" for each URL to the ThreadPoolExecutor.A critical aspect of this concurrent design is ensuring that all shared resources are thread-safe. The two primary shared resources in this framework are:The Rate-Limiter: The mechanism that throttles requests to a specific domain must work correctly across all threads. A simple time.sleep in each thread is not sufficient. A centralized, thread-safe rate-limiting object (using locks or a similar synchronization primitive) is required to ensure the total number of requests per second to a domain from all threads combined does not exceed the configured limit. Libraries like ratelimiter are designed to be thread-safe and are well-suited for this purpose.80The StateManager: The SQLite database connection must be handled correctly in a multithreaded context. While SQLite itself can handle concurrent writes, the best practice for using it with Python's sqlite3 module is to ensure that each thread uses its own connection object or that all writes are serialized through a single, dedicated writer thread or a thread-safe queue. The StateManager will be designed to manage this, ensuring that database updates from different worker threads do not conflict.By combining a ThreadPoolExecutor with thread-safe management of shared resources, the framework can scale its performance effectively while maintaining the integrity and robustness of its politeness and state management systems.Section 7: Packaging and Deployment GuideA well-designed framework is only useful if it is well-documented and easy to deploy. This final section provides the essential artifacts required to get the project up and running: a reproducible dependency list, comprehensive documentation, and example configuration files.7.1. The requirements.txt FileTo ensure that the scraping environment is consistent and reproducible across different machines and deployments, a requirements.txt file is essential. This file lists all external Python packages required by the project.For production stability, it is a best practice to "pin" the exact versions of all dependencies. This is achieved by using the == operator (e.g., requests==2.31.0). This prevents unexpected breakages that can occur when a dependency releases a new version with backward-incompatible changes. While developing, one might use >= to allow for minor updates, but for a stable, deployable artifact, exact versions are superior.81The requirements.txt file should be generated from a clean, dedicated virtual environment to avoid including unnecessary packages from the system's global Python installation. The pip freeze > requirements.txt command is the standard way to generate this file. An alternative tool, pipreqs, can generate the file by scanning the project's imports, which can result in a cleaner list of only direct dependencies, though it sometimes misses transitive dependencies.82 For maximum reproducibility, pip freeze from a clean environment is the most reliable method.The final requirements.txt for this framework will include:# requirements.txt

# Core HTTP and Parsing
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.2.1
trafilatura==1.9.0
html-sanitizer==1.11.1

# Configuration and State Management
PyYAML==6.0.1

# Politeness and Resilience
protego==0.4.0
tenacity==8.2.3
fake-useragent==1.5.1

# Validation and NLP
spacy==3.7.4
fasttext-wheel==0.9.2 # Using the wheel for easier installation
# Note: spaCy language models must be downloaded separately
# python -m spacy download es_core_news_sm

# Concurrency
# (concurrent.futures is in the standard library)

# Optional Headless Browser
playwright==1.44.0
# Note: Playwright browsers must be installed separately
# playwright install
7.2. The README.md File: Your Project's Front DoorThe README.md file is the most important piece of documentation for any software project. It is the first thing a new user or developer will see, and it should provide all the necessary information to understand, install, configure, and run the framework.1A comprehensive README.md for this project will be structured as follows:Robust Spanish-Language Web Scraping FrameworkThis project provides a robust, modular, and scalable Python-based framework for scraping high-quality, formal Spanish text from public websites. It is designed to generate clean, richly structured text corpora suitable for downstream Natural Language Processing (NLP) tasks, including the fine-tuning of Large Language Models (LLMs).FeaturesModular Architecture: A class-based design separates concerns for fetching, extraction, and saving, making the framework maintainable and extensible.Configuration-Driven: All operational parameters (sources, politeness, validation) are managed via external config.yaml and sources.yaml files, allowing for changes without modifying code.Robust Politeness Protocol:Honors robots.txt using the modern protego parser.Implements advanced request throttling with randomized delays.Rotates full, realistic browser header profiles to avoid detection.Uses exponential backoff with tenacity to handle transient server errors.High-Quality Extraction:Uses trafilatura as the primary engine for boilerplate removal and main content extraction.Includes a pre-emptive HTML sanitization step with html-sanitizer for corpus purity.Supports site-specific CSS selectors as a fallback using BeautifulSoup.Rigorous Validation:Performs high-accuracy language detection with fasttext to ensure a Spanish-only corpus.Validates content against minimum word and sentence counts using an optimized spaCy pipeline.Resilient and Scalable:Features a robust checkpointing and auto-resume system using a transactional SQLite database to handle interruptions.Utilizes multithreading (ThreadPoolExecutor) for high-throughput, I/O-bound scraping tasks.Dynamic Content Handling: Can optionally use Playwright to render JavaScript-heavy websites on a per-source basis.PrerequisitesPython 3.9+A C compiler (required for lxml and other dependencies)InstallationClone the repository:Bashgit clone https://github.com/your-repo/corpus-scraper.git
cd corpus-scraper
Create and activate a virtual environment:Bashpython -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
Install the required packages:Bashpip install -r requirements.txt
Download required NLP models:Bash# Download the spaCy Spanish language model
python -m spacy download es_core_news_sm

# Install Playwright's browser binaries (if you plan to use it)
playwright install
ConfigurationThe framework's behavior is controlled by two main files:config.yaml: This file contains global settings for politeness, validation, storage, and concurrency. Review and adjust the default values to suit your needs. Key settings include request_delay, min_word_count, and num_threads.sources.yaml: This file defines the websites to be scraped. Add or remove sources as needed. Each source requires a name and base_url. You can specify a sitemap_url for efficient URL discovery or a list of start_urls for crawling. Use the engine: playwright flag for sites that require JavaScript rendering.UsageTo start the scraping process, run the main.py script from the root directory of the project:Bashpython main.py
The framework will begin fetching URLs from the sources defined in sources.yaml, processing them according to the rules in config.yaml. Progress will be logged to the console and to data/logs/scraper.log. Cleaned text files will be saved to the data/corpus_raw/ directory.To resume an interrupted job, simply run the command again. The framework will automatically pick up from the last checkpoint stored in data/state/scraper_state.db.Ethical ConsiderationsThis framework is a powerful tool and should be used responsibly.Always check the robots.txt file and Terms of Service of any website before scraping. This framework is designed to respect robots.txt, but legal and ethical compliance is the user's responsibility.Do not overload servers. Use the request_delay and num_threads settings in config.yaml to maintain a respectful scraping rate. Be a good internet citizen.This tool is intended for use on publicly accessible data from government and academic sources for research purposes.7.3. Example Configuration FilesTo make the framework immediately usable, fully populated example configuration files are provided.config.yaml (Example):A file with sensible defaults, as detailed in Section 2.2.1, will be included in the project's root directory.sources.yaml (Example):This file will be pre-configured with the target sources requested by the user, demonstrating how to set up different types of sources.YAML# sources.yaml: Example list of target websites for corpus generation.

sources:
  - name: 'dof'
    base_url: 'https://dof.gob.mx'
    # The robots.txt for dof.gob.mx does not specify a sitemap.
    # We will provide start URLs for key sections.
    start_urls:
      - 'https://dof.gob.mx/' # Homepage to find daily editions
    # This site appears to be mostly static, so the default engine is appropriate.

  - name: 'scjn'
    base_url: 'https://www.scjn.gob.mx'
    # The robots.txt for scjn.gob.mx is inaccessible. We will crawl cautiously.
    start_urls:
      - 'https://www.scjn.gob.mx/multimedia'
      - 'https://www.scjn.gob.mx/conoce-la-corte/que-hace-la-scjn'
      - 'https://www.scjn.gob.mx/normativa-nacional-internacional'
    # Fallback selector for legal documents and press releases.
    fallback_selector: 'div.field-item.even'

  - name: 'unam'
    base_url: 'https://www.unam.mx'
    # UNAM is a vast domain; starting with key research and publication portals is wise.
    start_urls:
      - 'https://www.gaceta.unam.mx/'
      - 'https://www.investigacion.unam.mx/publicaciones'
      - 'https://www.cultura.unam.mx/publicaciones/'
    # Some UNAM sub-sites may use JS-heavy frameworks.
    # Setting the engine to playwright provides flexibility, though it could be
    # set on a more granular, sub-domain level if needed.
    engine: 'playwright'
This complete set of deliverables—code, dependencies, documentation, and configuration—ensures that the framework is not just a theoretical design but a practical, deployable, and immediately useful tool for the user's team.
